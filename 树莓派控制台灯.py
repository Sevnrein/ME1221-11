#!/usr/bin/env python3
"""
æ™ºèƒ½æƒ…ç»ªæ„Ÿåº”ç¯ - EfficientNet-B0 WiFiæ§åˆ¶æœ€ç»ˆç‰ˆ
ç¡¬ä»¶ï¼šæ ‘è“æ´¾4B + 500ä¸‡æ‘„åƒå¤´ + ç±³å®¶åºŠå¤´ç¯2 (MJTDo6YL)
åŠŸèƒ½ï¼šæ¯10åˆ†é’Ÿæ‹ç…§ä¸€æ¬¡ï¼Œè¯†åˆ«æƒ…ç»ªå¹¶æ§åˆ¶ç¯å…‰
"""

import time
import schedule
import cv2
import numpy as np
import tflite_runtime.interpreter as tflite
from miio import Yeelight
import logging
import sys
from datetime import datetime

# ============ é…ç½®åŒºåŸŸ (å¿…é¡»ä¿®æ”¹ï¼) ============
# 1. ç±³å®¶å°ç¯é…ç½® (é€šè¿‡WiFiæ§åˆ¶)
DEVICE_IP = "192.168.31.XXX"        # å°ç¯çš„å±€åŸŸç½‘IPåœ°å€
DEVICE_TOKEN = "æ‚¨çš„32ä½è®¾å¤‡ä»¤ç‰Œ"    # å°ç¯çš„Token

# 2. æ¨¡å‹é…ç½®
MODEL_PATH = "models/emotion_efficientnet_b0.tflite"  # TFLiteæ¨¡å‹è·¯å¾„
EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

# 3. æ‘„åƒå¤´é…ç½®
CAMERA_WIDTH = 640
CAMERA_HEIGHT = 480
CAPTURE_FRAMES = 3  # æ‹ç…§æ—¶è¿ç»­æ•è·å‡ å¸§ï¼ˆå–æœ€åä¸€å¸§ï¼Œè®©æ‘„åƒå¤´ç¨³å®šï¼‰

# 4. ç¨‹åºé…ç½®
LOG_FILE = "emotion_light.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
CHECK_INTERVAL = 10  # ä»»åŠ¡æ‰§è¡Œé—´éš”ï¼ˆåˆ†é’Ÿï¼‰

# ============ åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ ============
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ============ 1. TFLiteæƒ…ç»ªè¯†åˆ«å™¨ ============
class EfficientNetEmotionDetector:
    """ä½¿ç”¨TFLiteè¿è¡ŒEfficientNet-B0è¿›è¡Œæƒ…ç»ªè¯†åˆ«"""
    
    def __init__(self, model_path):
        """åˆå§‹åŒ–TFLiteè§£é‡Šå™¨å¹¶åŠ è½½æ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½TFLiteæ¨¡å‹: {model_path}")
            
            # åŠ è½½TFLiteæ¨¡å‹
            self.interpreter = tflite.Interpreter(model_path=model_path)
            self.interpreter.allocate_tensors()
            
            # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ…
            self.input_details = self.interpreter.get_input_details()
            self.output_details = self.interpreter.get_output_details()
            
            # è·å–è¾“å…¥å½¢çŠ¶
            input_shape = self.input_details[0]['shape']
            self.input_height, self.input_width = input_shape[1], input_shape[2]
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¾“å…¥å°ºå¯¸: {self.input_width}x{self.input_height}")
            logger.info(f"æ¨¡å‹è¾“å…¥è¯¦æƒ…: {self.input_details[0]}")
            logger.info(f"æ¨¡å‹è¾“å‡ºè¯¦æƒ…: {self.output_details[0]}")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def preprocess_image(self, image):
        """
        é¢„å¤„ç†å›¾åƒï¼Œè½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼
        
        æ³¨æ„ï¼šæ­¤å¤„çš„é¢„å¤„ç†å¿…é¡»ä¸è®­ç»ƒæ—¶çš„é¢„å¤„ç†å®Œå…¨ä¸€è‡´ï¼
        å¯¹äºEfficientNet-B0ï¼Œé€šå¸¸éœ€è¦ï¼š
        1. è°ƒæ•´å¤§å°åˆ°224x224
        2. åº”ç”¨EfficientNetç‰¹å®šçš„å½’ä¸€åŒ–
        """
        # è°ƒæ•´å¤§å°
        img_resized = cv2.resize(image, (self.input_width, self.input_height))
        
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼ï¼ˆå¦‚æœæ˜¯BGRåˆ™è½¬æ¢ï¼‰
        if len(img_resized.shape) == 3 and img_resized.shape[2] == 3:
            # OpenCVé»˜è®¤æ˜¯BGRï¼Œè½¬æ¢ä¸ºRGB
            img_resized = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
        
        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶å½’ä¸€åŒ–åˆ°[0, 1]
        img_normalized = img_resized.astype(np.float32) / 255.0
        
        # EfficientNetç‰¹å®šçš„é¢„å¤„ç†ï¼ˆImageNetå‡å€¼/æ ‡å‡†å·®ï¼‰
        # è¿™äº›å€¼å¿…é¡»ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„å€¼ä¸€è‡´ï¼
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        
        img_normalized[..., 0] = (img_normalized[..., 0] - mean[0]) / std[0]
        img_normalized[..., 1] = (img_normalized[..., 1] - mean[1]) / std[1]
        img_normalized[..., 2] = (img_normalized[..., 2] - mean[2]) / std[2]
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ [1, height, width, 3]
        img_batch = np.expand_dims(img_normalized, axis=0)
        
        return img_batch
    
    def predict_emotion(self, image):
        """
        å¯¹å•å¼ å›¾åƒè¿›è¡Œæƒ…ç»ªè¯†åˆ«
        
        è¿”å›: (æƒ…ç»ªæ ‡ç­¾, ç½®ä¿¡åº¦)
        """
        try:
            # é¢„å¤„ç†å›¾åƒ
            input_data = self.preprocess_image(image)
            
            # è®¾ç½®è¾“å…¥å¼ é‡
            self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
            
            # è¿è¡Œæ¨ç†
            start_time = time.time()
            self.interpreter.invoke()
            inference_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # è·å–è¾“å‡º
            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])
            
            # è§£æç»“æœ
            probabilities = output_data[0]
            predicted_class = np.argmax(probabilities)
            confidence = probabilities[predicted_class]
            
            emotion = EMOTION_LABELS[predicted_class]
            
            logger.debug(f"æ¨ç†æ—¶é—´: {inference_time:.1f}ms, æƒ…ç»ª: {emotion}, ç½®ä¿¡åº¦: {confidence:.2f}")
            
            return emotion, float(confidence)
            
        except Exception as e:
            logger.error(f"æƒ…ç»ªè¯†åˆ«å¤±è´¥: {e}")
            return "neutral", 0.0

# ============ 2. WiFiç¯å…‰æ§åˆ¶å™¨ ============
class WiFiLampController:
    """é€šè¿‡WiFiæ§åˆ¶ç±³å®¶åºŠå¤´ç¯2"""
    
    def __init__(self, ip, token):
        """åˆå§‹åŒ–ç¯å…‰æ§åˆ¶å™¨"""
        self.ip = ip
        self.token = token
        self.lamp = None
        self.is_connected = False
        self._connect()
    
    def _connect(self):
        """è¿æ¥åˆ°å°ç¯"""
        try:
            logger.info(f"æ­£åœ¨è¿æ¥å°ç¯ {self.ip}...")
            self.lamp = Yeelight(self.ip, self.token)
            
            # æµ‹è¯•è¿æ¥
            info = self.lamp.info()
            logger.info(f"âœ… å°ç¯è¿æ¥æˆåŠŸï¼å‹å·: {info.model}")
            self.is_connected = True
            
        except Exception as e:
            logger.error(f"âŒ å°ç¯è¿æ¥å¤±è´¥: {e}")
            logger.error("è¯·æ£€æŸ¥ï¼š1. IPåœ°å€æ˜¯å¦æ­£ç¡® 2. Tokenæ˜¯å¦æ­£ç¡® 3. å°ç¯æ˜¯å¦åœ¨çº¿")
            self.is_connected = False
    
    def set_emotion_light(self, emotion, confidence):
        """
        æ ¹æ®æƒ…ç»ªè®¾ç½®ç¯å…‰
        
        å‚æ•°:
            emotion: æƒ…ç»ªæ ‡ç­¾
            confidence: ç½®ä¿¡åº¦ (0.0-1.0)
        """
        if not self.is_connected:
            logger.warning("å°ç¯æœªè¿æ¥ï¼Œè·³è¿‡ç¯å…‰è®¾ç½®")
            return False
        
        try:
            # æƒ…ç»ªåˆ°ç¯å…‰å‚æ•°çš„æ˜ å°„ (æ ¹æ®ä½ çš„è¦æ±‚è°ƒæ•´)
            light_config = {
                'happy':     {'brightness': 85, 'rgb': (255, 200, 100)},    # å¼€å¿ƒ
                'neutral':   {'brightness': 65, 'rgb': (220, 230, 255)},    # å¹³é™
                'sad':       {'brightness': 45, 'rgb': (150, 180, 255)},    # ä½è½
                'angry':     {'brightness': 55, 'rgb': (255, 100, 100)},    # çƒ¦èº
                'surprise':  {'brightness': 70, 'rgb': (255, 255, 150)},    # æƒŠè®¶
                'fear':      {'brightness': 40, 'rgb': (100, 100, 200)},    # ææƒ§
                'disgust':   {'brightness': 50, 'rgb': (150, 200, 100)},    # åŒæ¶
            }
            
            # è·å–é…ç½®ï¼Œå¦‚æœæƒ…ç»ªæœªå®šä¹‰åˆ™ä½¿ç”¨ä¸­æ€§å…‰
            config = light_config.get(emotion, light_config['neutral'])
            brightness = config['brightness']
            rgb = config['rgb']
            
            # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´äº®åº¦ï¼ˆå¯é€‰ï¼‰
            # å¦‚æœç½®ä¿¡åº¦ä½äº0.5ï¼Œé™ä½äº®åº¦å˜åŒ–å¹…åº¦
            if confidence < 0.5:
                brightness = int(brightness * 0.7)
                logger.info(f"ç½®ä¿¡åº¦è¾ƒä½({confidence:.2f})ï¼Œä½¿ç”¨æŸ”å’Œç¯å…‰")
            
            # è½¬æ¢ä¸ºè®¾å¤‡èŒƒå›´ (0-255)
            device_brightness = int(brightness * 2.55)
            
            # è®¾ç½®ç¯å…‰
            self.lamp.set_rgb(rgb[0], rgb[1], rgb[2])
            time.sleep(0.05)  # çŸ­æš‚å»¶è¿Ÿ
            self.lamp.set_brightness(device_brightness)
            
            logger.info(f"ğŸ’¡ ç¯å…‰è®¾ç½®: {emotion} -> äº®åº¦{brightness}%, RGB{rgb}")
            return True
            
        except Exception as e:
            logger.error(f"è®¾ç½®ç¯å…‰å¤±è´¥: {e}")
            # å°è¯•é‡æ–°è¿æ¥
            try:
                self._connect()
            except:
                logger.error("é‡æ–°è¿æ¥å¤±è´¥")
            return False

# ============ 3. æ‘„åƒå¤´ç®¡ç†å™¨ ============
class CameraManager:
    """ç®¡ç†æ‘„åƒå¤´çš„æ•è·å’Œé‡Šæ”¾"""
    
    def __init__(self, width=640, height=480):
        self.width = width
        self.height = height
        self.cap = None
    
    def capture_image(self):
        """æ•è·ä¸€å¼ å›¾åƒ"""
        try:
            # å¦‚æœæ‘„åƒå¤´æœªæ‰“å¼€ï¼Œåˆ™æ‰“å¼€
            if self.cap is None:
                self.cap = cv2.VideoCapture(0)
                if not self.cap.isOpened():
                    logger.error("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
                    return None
                
                # è®¾ç½®æ‘„åƒå¤´å‚æ•°
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)
                logger.info(f"æ‘„åƒå¤´å·²æ‰“å¼€ï¼Œåˆ†è¾¨ç‡: {self.width}x{self.height}")
            
            # æ•è·å¤šå¸§ï¼Œè®©æ‘„åƒå¤´ç¨³å®šï¼ˆå–æœ€åä¸€å¸§ï¼‰
            for i in range(CAPTURE_FRAMES):
                ret, frame = self.cap.read()
                if not ret:
                    logger.error("æ•è·å›¾åƒå¤±è´¥")
                    self.release()
                    return None
            
            logger.debug(f"å›¾åƒæ•è·æˆåŠŸï¼Œå°ºå¯¸: {frame.shape}")
            return frame
            
        except Exception as e:
            logger.error(f"æ•è·å›¾åƒæ—¶å‡ºé”™: {e}")
            return None
    
    def release(self):
        """é‡Šæ”¾æ‘„åƒå¤´èµ„æº"""
        if self.cap is not None:
            self.cap.release()
            self.cap = None
            logger.debug("æ‘„åƒå¤´èµ„æºå·²é‡Šæ”¾")

# ============ 4. ä¸»ä»»åŠ¡å‡½æ•° ============
def emotion_detection_task():
    """ä¸»ä»»åŠ¡ï¼šæ•è·å›¾åƒã€è¯†åˆ«æƒ…ç»ªã€æ§åˆ¶ç¯å…‰"""
    logger.info("=" * 50)
    logger.info(f"å¼€å§‹æƒ…ç»ªè¯†åˆ«ä»»åŠ¡ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆå§‹åŒ–æ‘„åƒå¤´
    camera = CameraManager(CAMERA_WIDTH, CAMERA_HEIGHT)
    
    # æ•è·å›¾åƒ
    frame = camera.capture_image()
    if frame is None:
        logger.error("å›¾åƒæ•è·å¤±è´¥ï¼Œè·³è¿‡æœ¬æ¬¡ä»»åŠ¡")
        camera.release()
        return
    
    # å¯é€‰ï¼šä¿å­˜å›¾åƒç”¨äºè°ƒè¯•
    # cv2.imwrite(f"capture_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg", frame)
    
    # é‡Šæ”¾æ‘„åƒå¤´ï¼ˆé‡è¦ï¼šé•¿æ—¶é—´å ç”¨æ‘„åƒå¤´å¯èƒ½æœ‰é—®é¢˜ï¼‰
    camera.release()
    
    # æ£€æµ‹äººè„¸ï¼ˆå¯é€‰ï¼Œæé«˜å‡†ç¡®æ€§ï¼‰
    # å¦‚æœæ²¡æœ‰äººè„¸ï¼Œå¯ä»¥è·³è¿‡æƒ…ç»ªè¯†åˆ«
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    
    if len(faces) == 0:
        logger.warning("æœªæ£€æµ‹åˆ°äººè„¸ï¼Œè·³è¿‡æƒ…ç»ªè¯†åˆ«")
        # å¯ä»¥è®¾ç½®é»˜è®¤ç¯å…‰æˆ–ä¿æŒå½“å‰çŠ¶æ€
        return
    
    # è¯†åˆ«æƒ…ç»ª
    emotion, confidence = detector.predict_emotion(frame)
    
    # è®°å½•ç»“æœ
    logger.info(f"è¯†åˆ«ç»“æœ: {emotion} (ç½®ä¿¡åº¦: {confidence:.2%})")
    
    # æ§åˆ¶ç¯å…‰
    if confidence > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼
        lamp_controller.set_emotion_light(emotion, confidence)
    else:
        logger.warning(f"ç½®ä¿¡åº¦è¿‡ä½({confidence:.2%})ï¼Œä¸è°ƒæ•´ç¯å…‰")
    
    logger.info(f"ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {time.time() - task_start_time:.1f}ç§’")
    logger.info("=" * 50)

# ============ 5. ç¨‹åºå…¥å£ç‚¹ ============
if __name__ == "__main__":
    print("=" * 60)
    print("æ™ºèƒ½æƒ…ç»ªæ„Ÿåº”ç¯ - EfficientNet-B0 WiFiæ§åˆ¶ç‰ˆ")
    print(f"è¯†åˆ«é¢‘ç‡: æ¯{CHECK_INTERVAL}åˆ†é’Ÿä¸€æ¬¡")
    print("=" * 60)
    
    # å…¨å±€å˜é‡
    detector = None
    lamp_controller = None
    task_start_time = 0
    
    try:
        # åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨
        logger.info("åˆå§‹åŒ–æƒ…ç»ªæ£€æµ‹å™¨...")
        detector = EfficientNetEmotionDetector(MODEL_PATH)
        
        # åˆå§‹åŒ–ç¯å…‰æ§åˆ¶å™¨
        logger.info("åˆå§‹åŒ–ç¯å…‰æ§åˆ¶å™¨...")
        lamp_controller = WiFiLampController(DEVICE_IP, DEVICE_TOKEN)
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        logger.info(f"è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯{CHECK_INTERVAL}åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡...")
        schedule.every(CHECK_INTERVAL).minutes.do(
            lambda: emotion_detection_task()
        )
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡åˆå§‹ä»»åŠ¡
        logger.info("æ‰§è¡Œåˆå§‹æƒ…ç»ªè¯†åˆ«...")
        task_start_time = time.time()
        emotion_detection_task()
        
        logger.info("å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¿›å…¥ä¸»å¾ªç¯...")
        logger.info("æŒ‰ Ctrl+C é€€å‡ºç¨‹åº")
        print("\nç¨‹åºè¿è¡Œä¸­...")
        print("æƒ…ç»ªè¯†åˆ«æ—¥å¿—å°†æ˜¾ç¤ºåœ¨ä¸Šæ–¹å¹¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶")
        print("-" * 60)
        
        # ä¸»å¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ä»»åŠ¡
            
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œé”™è¯¯: {e}", exc_info=True)
    finally:
        # æ¸…ç†å·¥ä½œ
        logger.info("ç¨‹åºç»“æŸ")
        
        # è®¾ç½®ä¸€ä¸ªæŸ”å’Œçš„é»˜è®¤ç¯å…‰
        if lamp_controller and lamp_controller.is_connected:
            try:
                lamp_controller.lamp.set_rgb(255, 220, 180)
                lamp_controller.lamp.set_brightness(76)  # 30%äº®åº¦
                logger.info("å·²è®¾ç½®æŸ”å’Œé»˜è®¤ç¯å…‰")
            except:
                pass
